# app/xgboost.py

from fastapi import APIRouter, UploadFile, File, HTTPException
import pandas as pd
import numpy as np
import joblib
import json
import logging
from pathlib import Path
from io import BytesIO
from typing import Dict, Any

# plotting imports
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import base64

logger = logging.getLogger("app.xgboost")
router = APIRouter(prefix="/api/analyze/xgboost", tags=["XGBoost"])

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CONFIG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
MODEL_SAVE_DIR   = Path("saved_models_xgboost")
DEFAULT_MODEL_ID = "pretrained_xgboost"
TARGET_VARIABLES = [
    "d18O_measurement",
    "d13C_measurement",
    "Mg_Ca_measurement",
    "Sr_Ca_measurement",
]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def plot_to_base64(fig: plt.Figure) -> str:
    buf = BytesIO()
    fig.savefig(buf, format="png", dpi=80, bbox_inches="tight")
    plt.close(fig)
    return base64.b64encode(buf.getvalue()).decode("utf-8")

def create_residual_plot(y_true: np.ndarray, y_pred: np.ndarray, target: str) -> plt.Figure:
    fig, ax = plt.subplots(figsize=(6, 4))
    sns.histplot(y_true - y_pred, kde=True, bins=20, ax=ax)
    ax.set_title(f"{target} Residuals")
    ax.set_xlabel("Residual")
    return fig

def create_qq_plot(y_true: np.ndarray, y_pred: np.ndarray, target: str) -> plt.Figure:
    fig = plt.figure(figsize=(6, 4))
    stats.probplot(y_true - y_pred, dist="norm", plot=plt)
    plt.title(f"{target} QQ-Plot")
    return fig

def create_short_ts_plot(y_true: np.ndarray, y_pred: np.ndarray, target: str) -> plt.Figure:
    fig, ax = plt.subplots(figsize=(8, 4))
    ax.plot(y_true, label="Actual", alpha=0.7)
    ax.plot(y_pred, label="Predicted", alpha=0.7)
    ax.set_title(f"{target} Actual vs Predicted")
    ax.legend()
    return fig

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ PREDICTION ENDPOINT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@router.post("/predict")
async def predict_xgboost(
    file: UploadFile = File(...),
    model_id: str | None = None,
) -> Dict[str, Any]:
    contents = await file.read()
    try:
        df = pd.read_csv(BytesIO(contents))
    except Exception:
        raise HTTPException(400, "Uploaded file is not a valid CSV")
    if df.empty:
        raise HTTPException(400, "Uploaded file is empty")

    model_id = model_id or DEFAULT_MODEL_ID
    model_dir = MODEL_SAVE_DIR / model_id
    if not model_dir.exists():
        raise HTTPException(404, f"Model '{model_id}' not found")

    try:
        # ‚îÄ load metadata & metrics
        with open(model_dir / "metadata.json")    as f: metadata     = json.load(f)
        with open(model_dir / "test_metrics.json") as f: test_metrics = json.load(f)
        with open(model_dir / "cv_metrics.json")   as f: cv_metrics   = json.load(f)

        # sanitize test_metrics
        for t in TARGET_VARIABLES:
            tm = test_metrics.get(t, {})
            test_metrics[t] = {
                "r2":   float(tm.get("r2",   0.0)),
                "mae":  float(tm.get("mae",  0.0)),
                "rmse": float(tm.get("rmse", 0.0)),
            }

        # clean cross-val
        clean_cv = {
            t: v for t, v in cv_metrics.items()
            if isinstance(v, dict) and "mean_cv_score" in v
        }

        # ‚îÄ load preprocessors
        scaler         = joblib.load(model_dir / "scaler.pkl")
        label_encoders = joblib.load(model_dir / "label_encoders.pkl")

        # ‚îÄ preprocess incoming df
        drop_cols = ["sample_id", "site_id", "entity_id", "site_name"]
        df_proc = df.drop(columns=[c for c in drop_cols if c in df.columns], errors="ignore")
        num_cols = df_proc.select_dtypes(include=["float64", "int64"]).columns
        df_proc[num_cols] = df_proc[num_cols].fillna(df_proc[num_cols].median())
        for col, le in label_encoders.items():
            if col in df_proc:
                df_proc[col] = le.transform(df_proc[col].astype(str))
        df_proc[num_cols] = scaler.transform(df_proc[num_cols])

        X = df_proc.drop(columns=TARGET_VARIABLES, errors="ignore")

        # ‚îÄ predict per target & build plots
        future_past = {}
        plots       = {}
        for t in metadata.get("target_variables", TARGET_VARIABLES):
            pth = model_dir / f"XGB_{t}.pkl"
            if not pth.exists():
                logger.warning(f"[xgboost] Missing model file for '{t}'")
                continue

            model = joblib.load(pth)
            preds = model.predict(X)
            future_past[t] = preds.tolist()

            # get true values if available
            y_true = df_proc[t].values if t in df_proc else preds
            y_true_seq = y_true[: len(preds)]

            plots[t] = {
                "residual_plot":    plot_to_base64(create_residual_plot(y_true_seq, preds, t)),
                "qq_plot":          plot_to_base64(create_qq_plot(y_true_seq, preds, t)),
                "time_series_plot": plot_to_base64(create_short_ts_plot(y_true_seq, preds, t)),
            }

        # ‚îÄ build deep-time combined plot
        # span ‚Äì10000‚Üí+5000 with same number of points
        n = len(next(iter(future_past.values()), []))
        years = np.linspace(-10000, 5000, n)
        fig, ax = plt.subplots(figsize=(10, 5))
        for t, vals in future_past.items():
            ax.plot(years, vals, label=t)
        ax.axvline(0, color="r", linestyle="--", label="Present")
        ax.set_xlabel("Years (Past‚ÜíFuture)")
        ax.set_ylabel("Predicted Value")
        ax.legend()
        combined_ts = plot_to_base64(fig)

        # ‚îÄ all done, spicy log
        logger.info(f"üöÄ [xgboost] Quantum trees unleashed for '{model_id}' ‚Äî predictions served hot! üçæ")

        # ‚îÄ return payload
        return {
            "status": "success",
            "results": {
                "future_past_predictions":   future_past,
                "years":                     years.tolist(),
                "preprocessing":             metadata.get("preprocessing_info", {}),
                "training_metrics":          test_metrics,
                "evaluation_metrics":        test_metrics,
                "training_cross_validation": clean_cv,
                "plots":                     plots,
                "time_series_plot":          combined_ts,
            },
            "model_id": model_id,
        }

    except Exception as e:
        logger.error(f"[xgboost] load error: {e}", exc_info=True)
        raise HTTPException(400, f"Error loading model: {e}")
